{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8abff1c-c7f3-4ae1-bb4d-c20eff74566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best solution (genes): [ 1.  1.  1.  2.  1.  3.  1.  6.  1.  8.  1. 17.  2.  1.  1.  1.  2.  1.\n",
      "  4.  3.  6.  1.  1.  1.  3.  1.  4.  1.  6.]\n",
      "------------------------------------------------------------------------------\n",
      "Original Test Results\n",
      "------------------------------------------------------------------------------\n",
      "AUC: 0.6764950166112956\n",
      "Optimal Threshold: 10.0\n",
      "Max Youden Index: 0.22093023255813948\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.22093023255813954\n",
      "PPV: 0.0945945945945946\n",
      "NPV: 1.0\n",
      "Brier Score: 0.07304237491936502\n",
      "ECE: 0.06926645102125673\n",
      "Hosmer-Lemeshow p-value: 0.11452784320005827\n",
      "NRI: 0.0\n",
      "IDI: 0.0\n",
      "------------------------------------------------------------------------------\n",
      "Test Results\n",
      "------------------------------------------------------------------------------\n",
      "AUC: 0.6879152823920266\n",
      "Optimal Threshold: 9.0\n",
      "Max Youden Index: 0.34883720930232553\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.3488372093023256\n",
      "PPV: 0.1111111111111111\n",
      "NPV: 1.0\n",
      "Brier Score: 0.06897768399502605\n",
      "ECE: 0.023091504175416208\n",
      "Hosmer-Lemeshow p-value: 0.9842916096651328\n",
      "NRI: 0.4667774086378737\n",
      "IDI: 0.03208486781565958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pygad\n",
    "from utils import compute_metrics, roc_auc_score, compute_predicted_probabilities, compute_nri, make_prob_dict\n",
    "\n",
    "global_fitness_history = {}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# ------------------------------------------------------------------------------\n",
    "np.random.seed(42)  # Seed for NumPy random operations\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Load the dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "full_data = pd.read_csv(\"lace.csv\")\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.4, random_state=42, stratify = full_data['30Readmit'])\n",
    "\n",
    "for mode in [\"train\",\"test\",\"full\"]:\n",
    "    exec(f\"{mode}_LOS = {mode}_data['Length of stay (days)'].values \")\n",
    "    exec(f\"{mode}_Acute = {mode}_data['Acute (emergent) admission'].values\")\n",
    "    exec(f\"{mode}_Charlson = {mode}_data['Charlson Comorbidity Index'].values\")\n",
    "    exec(f\"{mode}_ED = {mode}_data['Number of ED visits within 6 months'].values\")\n",
    "    exec(f\"{mode}_y = {mode}_data['30Readmit'].values\")\n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Define the original LACE scoring function (baseline)\n",
    "# ------------------------------------------------------------------------------\n",
    "W = [1,1,1,2,1,3,1,4,1,7,2,14,\n",
    "     3,\n",
    "     1,1,1,2,1,3,2,4,\n",
    "     1,1,1,2,1,3,1,4]\n",
    "\n",
    "def scoring(w_L1, t_L1, w_L2, t_L2, w_L3, t_L3, w_L4, t_L4, w_L5, t_L5, w_L6, t_L6, w_A, w_C1, t_C1, w_C2, t_C2, w_C3, t_C3, w_C4, t_C4, w_E1, t_E1, w_E2, t_E2, w_E3, t_E3, w_E4, t_E4, mode=\"train\"):\n",
    "    LOS = eval(f\"{mode}_LOS\")\n",
    "    Acute = eval(f\"{mode}_Acute\")\n",
    "    Charlson = eval(f\"{mode}_Charlson\")\n",
    "    ED = eval(f\"{mode}_ED\")\n",
    "\n",
    "    local_vars = dict(locals())\n",
    "    score_LOS = sum( [eval(f\"w_L{i} * (LOS>=t_L{i})\", globals(), local_vars) for i in range(1,7) ] )\n",
    "    score_Acute = w_A * (Acute > 0.5)\n",
    "    score_Charlson = sum( [eval(f\"w_C{i} * (Charlson>=t_C{i})\", globals(), local_vars) for i in range(1,5) ] )\n",
    "    score_ED = sum( [eval(f\"w_E{i} * (ED>=t_E{i})\", globals(), local_vars) for i in range(1,5) ] )\n",
    "    total_score = score_LOS + score_Acute + score_Charlson + score_ED\n",
    "    return total_score\n",
    "    \n",
    "# Compute the baseline (original) scores and their AUC\n",
    "orig_scores = scoring(*W)\n",
    "orig_auc = roc_auc_score(train_y, orig_scores)\n",
    "test_orig_scores = scoring(*W, mode = \"test\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Define the fitness function for candidate solutions\n",
    "# ------------------------------------------------------------------------------\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    global global_fitness_history\n",
    "    if tuple(solution) in global_fitness_history:\n",
    "        return global_fitness_history[tuple(solution)]\n",
    "        \n",
    "    # Unpack genes\n",
    "    w_L1, t_L1, w_L2, t_L2, w_L3, t_L3, w_L4, t_L4, w_L5, t_L5, w_L6, t_L6, w_A, w_C1, t_C1, w_C2, t_C2, w_C3, t_C3, w_C4, t_C4, w_E1, t_E1, w_E2, t_E2, w_E3, t_E3, w_E4, t_E4 = solution\n",
    "    \n",
    "    heavy_constant = 1  # Constant to heavily penalize ordering violations\n",
    "    light_constant = 0.05 # Constant to lightly penalize deviation from original score \n",
    "    \n",
    "    # Apply penalty only when the constraint is violated:\n",
    "    penalty = heavy_constant * max(0, t_L1 - (t_L2-1))\n",
    "    penalty += heavy_constant * max(0, t_L2 - (t_L3-1))\n",
    "    penalty += heavy_constant * max(0, t_L3 - (t_L4-1))\n",
    "    penalty += heavy_constant * max(0, t_L4 - (t_L5-1))\n",
    "    penalty += heavy_constant * max(0, t_L4 - (t_L5-1))\n",
    "    penalty += heavy_constant * max(0, t_L5 - (t_L6-1))\n",
    "    penalty += heavy_constant * max(0, t_C1 - (t_C2-1))\n",
    "    penalty += heavy_constant * max(0, t_C2 - (t_C3-1))\n",
    "    penalty += heavy_constant * max(0, t_C3 - (t_C4-1))\n",
    "    penalty += heavy_constant * max(0, t_E1 - (t_E2-1))\n",
    "    penalty += heavy_constant * max(0, t_E2 - (t_E3-1))\n",
    "    penalty += heavy_constant * max(0, t_E3 - (t_E4-1))\n",
    "    penalty += light_constant * np.average([np.abs(i-j)/i for i,j in zip(W,solution)]) # Average value of the ratio of the change\n",
    "    \n",
    "    # Compute candidate's score using indicator functions:\n",
    "    candidate_score = scoring(*solution)\n",
    "\n",
    "    try:\n",
    "        # --- Objective 1: Active AUC Improvement ---\n",
    "        candidate_auc = roc_auc_score(train_y, candidate_score)\n",
    "        active_auc = candidate_auc - orig_auc\n",
    "        \n",
    "        # --- Objective 2: Active Brier Score Improvement ---\n",
    "        baseline_probs = compute_predicted_probabilities(orig_scores, train_y)\n",
    "        candidate_probs = compute_predicted_probabilities(candidate_score, train_y)\n",
    "        brier_candidate = np.mean((candidate_probs - np.array(train_y)) ** 2)\n",
    "        brier_baseline = np.mean((baseline_probs - np.array(train_y)) ** 2)\n",
    "        active_brier = brier_baseline - brier_candidate\n",
    "        \n",
    "        # --- Objective 3: Net Reclassification Improvement (NRI) ---\n",
    "        nri = compute_nri(baseline_probs, candidate_probs, train_y)\n",
    "        \n",
    "        obj1 = max(0,active_auc - penalty)\n",
    "        obj2 = max(0,active_brier - penalty)\n",
    "        obj3 = 10**-6 if nri>0 else 0\n",
    "\n",
    "    except ValueError:\n",
    "        obj1, obj2, obj3 = -0.01, -0.01, -0.01\n",
    "    \n",
    "    # Store the fitness vector in the global dictionary.\n",
    "    global_fitness_history[tuple(solution)] = np.array([obj1, obj2, obj3])\n",
    "    \n",
    "    # Return the vector of objectives.\n",
    "    return np.array([obj1, obj2, obj3])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Define the gene space for integer solutions\n",
    "# ------------------------------------------------------------------------------\n",
    " # Original score, with lower limit of 1 and upper limit that is 1.5x of the current value + 1 \n",
    "\n",
    "gene_space = [list(range(1,i//2+i+1)) for i in W] \n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Set up and run the Genetic Algorithm with PyGAD\n",
    "# ------------------------------------------------------------------------------\n",
    "ga_instance = pygad.GA(\n",
    "    num_generations=100,         \n",
    "    num_parents_mating=100,   \n",
    "    fitness_func=fitness_func,   \n",
    "    sol_per_pop=500,          \n",
    "    num_genes=len(gene_space),  \n",
    "    gene_space=gene_space,       \n",
    "    mutation_probability=0.2,   \n",
    "    random_seed=0,          \n",
    "    initial_population = [W]*100,\n",
    "    parent_selection_type='nsga2',\n",
    ")\n",
    "\n",
    "ga_instance.run()\n",
    "\n",
    "# Retrieve the best solution\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Best solution (genes):\", solution)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Report the AUC of the best candidate solution\n",
    "# ------------------------------------------------------------------------------\n",
    "# Recompute candidate score for the best solution\n",
    "\n",
    "best_candidate_score = scoring(*solution)\n",
    "train_baseline_prob_dict = make_prob_dict(orig_scores, train_y)\n",
    "train_prob_dict = make_prob_dict(best_candidate_score, train_y)\n",
    "test_best_candidate_score = scoring(*solution, mode = 'test')\n",
    "_, train_threshold = compute_metrics(orig_scores, train_y, train_prob_dict=train_baseline_prob_dict)\n",
    "_, new_train_threshold = compute_metrics(best_candidate_score, train_y, baseline_scores=orig_scores, train_prob_dict=train_prob_dict,train_baseline_prob_dict=train_baseline_prob_dict)\n",
    "\n",
    "print(\"------------------------------------------------------------------------------\\nOriginal Test Results\\n------------------------------------------------------------------------------\")\n",
    "results = compute_metrics(test_orig_scores, test_y, train_threshold=train_threshold, baseline_scores=test_orig_scores, train_prob_dict=train_baseline_prob_dict, train_baseline_prob_dict=train_baseline_prob_dict)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"------------------------------------------------------------------------------\\nTest Results\\n------------------------------------------------------------------------------\")\n",
    "results = compute_metrics(test_best_candidate_score, test_y, train_threshold=new_train_threshold, baseline_scores=test_orig_scores, train_prob_dict=train_prob_dict, train_baseline_prob_dict=train_baseline_prob_dict)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
